{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b8a16fd69d4f4ccaa30109be6ac50880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c2e6af6e4bf417f9c744d48d6395412",
              "IPY_MODEL_1514114962e14efb8a2034cf9e6bdf06",
              "IPY_MODEL_01c1a958b4c6428ea2d40f85d14728dd"
            ],
            "layout": "IPY_MODEL_7bf0401b5fa74ca1bd1134b9e8fec221"
          }
        },
        "6c2e6af6e4bf417f9c744d48d6395412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_495cafb13dbc405c87bf8860e06c86d3",
            "placeholder": "​",
            "style": "IPY_MODEL_4e2c23c2e5e447558b65e88fcbab29db",
            "value": "100%"
          }
        },
        "1514114962e14efb8a2034cf9e6bdf06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f90c14545c6473d98798556e2219a9f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab3f8ea56a584aeaa126dec8f0edb1c8",
            "value": 3
          }
        },
        "01c1a958b4c6428ea2d40f85d14728dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad47f6154fe145c6856534c5b3597282",
            "placeholder": "​",
            "style": "IPY_MODEL_cb190399aa9b4c349e37853555a7b5f2",
            "value": " 3/3 [00:00&lt;00:00,  5.75it/s]"
          }
        },
        "7bf0401b5fa74ca1bd1134b9e8fec221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "495cafb13dbc405c87bf8860e06c86d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2c23c2e5e447558b65e88fcbab29db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f90c14545c6473d98798556e2219a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab3f8ea56a584aeaa126dec8f0edb1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad47f6154fe145c6856534c5b3597282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb190399aa9b4c349e37853555a7b5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aa22860df54419ba25a20b865cd8e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ba6c4dbb5554d048fea87dd0b6d6cec",
              "IPY_MODEL_8f3fda2197b6447ab609f7d7e4dfaf9b",
              "IPY_MODEL_9e66885cd12d4c7aa3a6862dfcde2b4b"
            ],
            "layout": "IPY_MODEL_85e52fb30ac74f85a2cd5a6adfd96ecc"
          }
        },
        "5ba6c4dbb5554d048fea87dd0b6d6cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e167b521be64744bb14ea93d45bf6e7",
            "placeholder": "​",
            "style": "IPY_MODEL_67cd814d4b7e424eb34d1630d40c2022",
            "value": "Map: 100%"
          }
        },
        "8f3fda2197b6447ab609f7d7e4dfaf9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dabbe81bbfe44de9942323ab28b94ca",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_963d8cf619924c088bd1057fddc1644e",
            "value": 25000
          }
        },
        "9e66885cd12d4c7aa3a6862dfcde2b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12608a66b9c94035928251aa31cb3d1a",
            "placeholder": "​",
            "style": "IPY_MODEL_01bf3d186abd48559d658ed92d65fe92",
            "value": " 25000/25000 [00:42&lt;00:00, 797.81 examples/s]"
          }
        },
        "85e52fb30ac74f85a2cd5a6adfd96ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "5e167b521be64744bb14ea93d45bf6e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67cd814d4b7e424eb34d1630d40c2022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dabbe81bbfe44de9942323ab28b94ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963d8cf619924c088bd1057fddc1644e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12608a66b9c94035928251aa31cb3d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01bf3d186abd48559d658ed92d65fe92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CMPE -297**\n",
        "\n",
        "NLP Assignment\n",
        "\n",
        "Aditya Sahu "
      ],
      "metadata": {
        "id": "c-QSYEbN9nak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, BERT for question answering model will be explored from the huggingface library"
      ],
      "metadata": {
        "id": "DKM_B0lyVgZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Part 1 of the demo, we will use a fine-tuned BERT on the **SQuAD** dataset and apply it (test) it) on the **CoQA** dataset.  In Part 2 of the demo you will learn how to fine tune BERT for question answering on the **SQuAD** dataset yourselves."
      ],
      "metadata": {
        "id": "COjyQMpmVuKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1"
      ],
      "metadata": {
        "id": "2P_BZlt6WJDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization & Setup\n"
      ],
      "metadata": {
        "id": "tJJVMPRUeZpY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "hIt7JVfsh-x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d87e058-30fe-45fb-9d28-3f79949b4830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "_7mkVnaXiELh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the CoQA dataset"
      ],
      "metadata": {
        "id": "NzCTmMjpenBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
        "coqa"
      ],
      "metadata": {
        "id": "gh8h6Q2biYtI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "acffa242-cb4d-4132-e682-c30fa6d7549c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      version                                               data\n",
              "0           1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
              "1           1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
              "2           1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
              "3           1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
              "4           1  {'source': 'gutenberg', 'id': '3urfvvm165iantk...\n",
              "...       ...                                                ...\n",
              "7194        1  {'source': 'gutenberg', 'id': '34j10vatjfyw0ao...\n",
              "7195        1  {'source': 'cnn', 'id': '3vj40nv2qinjocrcy7k4z...\n",
              "7196        1  {'source': 'race', 'id': '3rjsc4xj10uw0to3vq0v...\n",
              "7197        1  {'source': 'wikipedia', 'id': '3gs6s824sqxty8v...\n",
              "7198        1  {'source': 'cnn', 'id': '31qnsg6a5rtt5m7pens7x...\n",
              "\n",
              "[7199 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5526df7d-4210-4a6c-8dc3-5b915e0b453b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7194</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'gutenberg', 'id': '34j10vatjfyw0ao...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7195</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'cnn', 'id': '3vj40nv2qinjocrcy7k4z...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7196</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'race', 'id': '3rjsc4xj10uw0to3vq0v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7197</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'wikipedia', 'id': '3gs6s824sqxty8v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7198</th>\n",
              "      <td>1</td>\n",
              "      <td>{'source': 'cnn', 'id': '31qnsg6a5rtt5m7pens7x...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7199 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5526df7d-4210-4a6c-8dc3-5b915e0b453b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5526df7d-4210-4a6c-8dc3-5b915e0b453b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5526df7d-4210-4a6c-8dc3-5b915e0b453b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coqa[\"data\"][0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UeY19INhvd5",
        "outputId": "a8ddca9c-e37a-47f7-d1d4-58b256246fd9"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['source', 'id', 'filename', 'story', 'questions', 'answers', 'name'])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the data"
      ],
      "metadata": {
        "id": "6qVbBwy5exBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coqa[\"data\"][0]"
      ],
      "metadata": {
        "id": "0zq8-6qLp61F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87110285-aebc-4007-e275-8648c398035b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'wikipedia',\n",
              " 'id': '3zotghdk5ibi9cex97fepx7jetpso7',\n",
              " 'filename': 'Vatican_Library.txt',\n",
              " 'story': 'The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.',\n",
              " 'questions': [{'input_text': 'When was the Vat formally opened?',\n",
              "   'turn_id': 1},\n",
              "  {'input_text': 'what is the library for?', 'turn_id': 2},\n",
              "  {'input_text': 'for what subjects?', 'turn_id': 3},\n",
              "  {'input_text': 'and?', 'turn_id': 4},\n",
              "  {'input_text': 'what was started in 2014?', 'turn_id': 5},\n",
              "  {'input_text': 'how do scholars divide the library?', 'turn_id': 6},\n",
              "  {'input_text': 'how many?', 'turn_id': 7},\n",
              "  {'input_text': 'what is the official name of the Vat?', 'turn_id': 8},\n",
              "  {'input_text': 'where is it?', 'turn_id': 9},\n",
              "  {'input_text': 'how many printed books does it contain?', 'turn_id': 10},\n",
              "  {'input_text': 'when were the Secret Archives moved from the rest of the library?',\n",
              "   'turn_id': 11},\n",
              "  {'input_text': 'how many items are in this secret collection?',\n",
              "   'turn_id': 12},\n",
              "  {'input_text': 'Can anyone use this library?', 'turn_id': 13},\n",
              "  {'input_text': 'what must be requested to view?', 'turn_id': 14},\n",
              "  {'input_text': 'what must be requested in person or by mail?',\n",
              "   'turn_id': 15},\n",
              "  {'input_text': 'of what books?', 'turn_id': 16},\n",
              "  {'input_text': 'What is the Vat the library of?', 'turn_id': 17},\n",
              "  {'input_text': 'How many books survived the Pre Lateran period?',\n",
              "   'turn_id': 18},\n",
              "  {'input_text': 'what is the point of the project started in 2014?',\n",
              "   'turn_id': 19},\n",
              "  {'input_text': 'what will this allow?', 'turn_id': 20}],\n",
              " 'answers': [{'span_start': 151,\n",
              "   'span_end': 179,\n",
              "   'span_text': 'Formally established in 1475',\n",
              "   'input_text': 'It was formally established in 1475',\n",
              "   'turn_id': 1},\n",
              "  {'span_start': 454,\n",
              "   'span_end': 494,\n",
              "   'span_text': 'he Vatican Library is a research library',\n",
              "   'input_text': 'research',\n",
              "   'turn_id': 2},\n",
              "  {'span_start': 457,\n",
              "   'span_end': 511,\n",
              "   'span_text': 'Vatican Library is a research library for history, law',\n",
              "   'input_text': 'history, and law',\n",
              "   'turn_id': 3},\n",
              "  {'span_start': 457,\n",
              "   'span_end': 545,\n",
              "   'span_text': 'Vatican Library is a research library for history, law, philosophy, science and theology',\n",
              "   'input_text': 'philosophy, science and theology',\n",
              "   'turn_id': 4},\n",
              "  {'span_start': 769,\n",
              "   'span_end': 879,\n",
              "   'span_text': 'March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts',\n",
              "   'input_text': 'a  project',\n",
              "   'turn_id': 5},\n",
              "  {'span_start': 1048,\n",
              "   'span_end': 1127,\n",
              "   'span_text': 'Scholars have traditionally divided the history of the library into five period',\n",
              "   'input_text': 'into periods',\n",
              "   'turn_id': 6},\n",
              "  {'span_start': 1048,\n",
              "   'span_end': 1128,\n",
              "   'span_text': 'Scholars have traditionally divided the history of the library into five periods',\n",
              "   'input_text': 'five',\n",
              "   'turn_id': 7},\n",
              "  {'span_start': 4,\n",
              "   'span_end': 94,\n",
              "   'span_text': 'Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, ',\n",
              "   'input_text': 'The Vatican Apostolic Library',\n",
              "   'turn_id': 8},\n",
              "  {'span_start': 94,\n",
              "   'span_end': 150,\n",
              "   'span_text': 'is the library of the Holy See, located in Vatican City.',\n",
              "   'input_text': 'in Vatican City',\n",
              "   'turn_id': 9},\n",
              "  {'span_start': 328,\n",
              "   'span_end': 412,\n",
              "   'span_text': ' It has 75,000 codices from throughout history, as well as 1.1 million printed books',\n",
              "   'input_text': '1.1 million',\n",
              "   'turn_id': 10},\n",
              "  {'span_start': 917,\n",
              "   'span_end': 1009,\n",
              "   'span_text': 'atican Secret Archives were separated from the library at the beginning of the 17th century;',\n",
              "   'input_text': 'at the beginning of the 17th century;',\n",
              "   'turn_id': 11},\n",
              "  {'span_start': 915,\n",
              "   'span_end': 1046,\n",
              "   'span_text': ' Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. ',\n",
              "   'input_text': '150,000',\n",
              "   'turn_id': 12},\n",
              "  {'span_start': 546,\n",
              "   'span_end': 643,\n",
              "   'span_text': ' The Vatican Library is open to anyone who can document their qualifications and research needs. ',\n",
              "   'input_text': 'anyone who can document their qualifications and research needs.',\n",
              "   'turn_id': 13},\n",
              "  {'span_start': -1,\n",
              "   'span_end': -1,\n",
              "   'span_text': 'unknown',\n",
              "   'input_text': 'unknown',\n",
              "   'turn_id': 14,\n",
              "   'bad_turn': 'true'},\n",
              "  {'span_start': 643,\n",
              "   'span_end': 764,\n",
              "   'span_text': 'Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. ',\n",
              "   'input_text': 'Photocopies',\n",
              "   'turn_id': 15},\n",
              "  {'span_start': 644,\n",
              "   'span_end': 724,\n",
              "   'span_text': 'hotocopies for private study of pages from books published between 1801 and 1990',\n",
              "   'input_text': 'only books published between 1801 and 1990',\n",
              "   'turn_id': 16},\n",
              "  {'span_start': 78,\n",
              "   'span_end': 125,\n",
              "   'span_text': 'simply the Vat, is the library of the Holy See,',\n",
              "   'input_text': 'the Holy See',\n",
              "   'turn_id': 17},\n",
              "  {'span_start': 1192,\n",
              "   'span_end': 1384,\n",
              "   'span_text': 'Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant',\n",
              "   'input_text': 'a handful of volumes',\n",
              "   'turn_id': 18},\n",
              "  {'span_start': 785,\n",
              "   'span_end': 881,\n",
              "   'span_text': 'Vatican Library began an initial four-year project of digitising its collection of manuscripts, ',\n",
              "   'input_text': 'digitising manuscripts',\n",
              "   'turn_id': 19},\n",
              "  {'span_start': 868,\n",
              "   'span_end': 910,\n",
              "   'span_text': 'manuscripts, to be made available online. ',\n",
              "   'input_text': 'them to be viewed online.',\n",
              "   'turn_id': 20}],\n",
              " 'name': 'Vatican_Library.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **CoQA** dataset contains ~7200 rows, and each row contains one paragraph and multiple question and answer pairs related to that paragraph.\n",
        "\n",
        "If we print the first row, we see that there are 20 questions and answers for the first paragraph and that answers are in the form of start index and end index within the paragraph.  This is the standard format of any closed domain question answering dataset."
      ],
      "metadata": {
        "id": "lWzznLgFXB8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deleting an unnecessary column\n",
        "del coqa[\"version\"]\n",
        "coqa"
      ],
      "metadata": {
        "id": "C5DqWRgYiwjq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "e8ddc402-69b0-4aee-bad4-931af9ef5869"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   data\n",
              "0     {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
              "1     {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
              "2     {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
              "3     {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
              "4     {'source': 'gutenberg', 'id': '3urfvvm165iantk...\n",
              "...                                                 ...\n",
              "7194  {'source': 'gutenberg', 'id': '34j10vatjfyw0ao...\n",
              "7195  {'source': 'cnn', 'id': '3vj40nv2qinjocrcy7k4z...\n",
              "7196  {'source': 'race', 'id': '3rjsc4xj10uw0to3vq0v...\n",
              "7197  {'source': 'wikipedia', 'id': '3gs6s824sqxty8v...\n",
              "7198  {'source': 'cnn', 'id': '31qnsg6a5rtt5m7pens7x...\n",
              "\n",
              "[7199 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8c0b112-2af4-4794-9189-9fd6e880dde8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7194</th>\n",
              "      <td>{'source': 'gutenberg', 'id': '34j10vatjfyw0ao...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7195</th>\n",
              "      <td>{'source': 'cnn', 'id': '3vj40nv2qinjocrcy7k4z...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7196</th>\n",
              "      <td>{'source': 'race', 'id': '3rjsc4xj10uw0to3vq0v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7197</th>\n",
              "      <td>{'source': 'wikipedia', 'id': '3gs6s824sqxty8v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7198</th>\n",
              "      <td>{'source': 'cnn', 'id': '31qnsg6a5rtt5m7pens7x...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7199 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8c0b112-2af4-4794-9189-9fd6e880dde8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8c0b112-2af4-4794-9189-9fd6e880dde8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8c0b112-2af4-4794-9189-9fd6e880dde8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the CoQA dataset to a more convenient format\n",
        "We convert the CoQA dataset to a more convenient format by creating one question and answer pair per row.  This results in repeated content in the \"text\" column - once per questions and answer for the respective paragraph, we will be repeating the paragraph in the \"text\" column. "
      ],
      "metadata": {
        "id": "J4wZ2SFSd4A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"text\",\"question\",\"answer\"]\n",
        "comp_list = []\n",
        "for index, row in coqa.iterrows():\n",
        "    for i in range(len(row[\"data\"][\"questions\"])):\n",
        "        temp_list = []\n",
        "        temp_list.append(row[\"data\"][\"story\"])\n",
        "        temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
        "        temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
        "        comp_list.append(temp_list)\n",
        "new_df = pd.DataFrame(comp_list, columns = cols) \n",
        "new_df"
      ],
      "metadata": {
        "id": "HY0TXR7WkGxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "90a4e98e-390e-4c3c-f155-afec1f2eab05"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  \\\n",
              "0       The Vatican Apostolic Library (), more commonl...   \n",
              "1       The Vatican Apostolic Library (), more commonl...   \n",
              "2       The Vatican Apostolic Library (), more commonl...   \n",
              "3       The Vatican Apostolic Library (), more commonl...   \n",
              "4       The Vatican Apostolic Library (), more commonl...   \n",
              "...                                                   ...   \n",
              "108642  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "108643  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "108644  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "108645  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "108646  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "\n",
              "                                 question                               answer  \n",
              "0       When was the Vat formally opened?  It was formally established in 1475  \n",
              "1                what is the library for?                             research  \n",
              "2                      for what subjects?                     history, and law  \n",
              "3                                    and?     philosophy, science and theology  \n",
              "4               what was started in 2014?                           a  project  \n",
              "...                                   ...                                  ...  \n",
              "108642                     Who was a sub?                          Xabi Alonso  \n",
              "108643   Was it his first game this year?                                  Yes  \n",
              "108644  What position did the team reach?                                third  \n",
              "108645             Who was ahead of them?                               Barca.  \n",
              "108646                       By how much?                           six points  \n",
              "\n",
              "[108647 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c61418c3-4585-40a2-b561-7483e8d487ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>When was the Vat formally opened?</td>\n",
              "      <td>It was formally established in 1475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what is the library for?</td>\n",
              "      <td>research</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>for what subjects?</td>\n",
              "      <td>history, and law</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>and?</td>\n",
              "      <td>philosophy, science and theology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what was started in 2014?</td>\n",
              "      <td>a  project</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108642</th>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>Who was a sub?</td>\n",
              "      <td>Xabi Alonso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108643</th>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>Was it his first game this year?</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108644</th>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>What position did the team reach?</td>\n",
              "      <td>third</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108645</th>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>Who was ahead of them?</td>\n",
              "      <td>Barca.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108646</th>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>By how much?</td>\n",
              "      <td>six points</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>108647 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c61418c3-4585-40a2-b561-7483e8d487ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c61418c3-4585-40a2-b561-7483e8d487ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c61418c3-4585-40a2-b561-7483e8d487ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading BERT fine-tuned on SQuAD\n",
        "Loading BERT for question answering which is already fine-tuned on SQuAD, as well as the corresponding BERT tokenizer (each pre-trained BERT model has a corresponding tokenizer)\n"
      ],
      "metadata": {
        "id": "8cCcP23jeEBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "vkhE7EJEkX0X"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with BERT\n"
      ],
      "metadata": {
        "id": "lnrcTHkXYX29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# picking out a random question and answer pair from the dataset\n",
        "random_num = np.random.randint(0,len(new_df))\n",
        "question = new_df[\"question\"][random_num]\n",
        "text = new_df[\"text\"][random_num]\n",
        "print(\"Question: \", question)\n",
        "print(\"Text: \", text)"
      ],
      "metadata": {
        "id": "jfCE9N-tksmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac55c6b-253d-45d8-9499-fbc41df51e9b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What did he say about the experience?\n",
            "Text:  How much do you know about Albert Einstein? Albert Einstein, born on March 14, 1579 in Germany, was a great scientist in the world. He was strange because he hated haircuts and new clothes. He believed in peace. All his life, he hated war. However, his most famous idea, E=mc2, helped create the world's most dangerous weapon . Many people think he was the smartest person in the world. But Einstein said that _ What did he like? Einstein liked learning sailing . He sailed in small boats all his life. He once joked, \"Sailing is the sport that takes the least energy!\" When Einstein was a child, his mother made him take violin lessons. At first, he didn't like the violin. But then he learned to love music and became a good violinist. Later, he said, \"Love is the best teacher.\" Why is the sky blue? In 1910, Einstein asked a question which many children often ask, \"Why is the sky blue?\" After his careful research, he answered the question like this: \"It's because light is made up of many colors including blue. When light travels to Earth, gas particles spread the blue light all over the sky.\" His answer is true in physics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"answer\"][random_num]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CsZpe-q_Ere3",
        "outputId": "19cad08e-24ee-46b0-b133-7f5c8f1f0755"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Love is the best teacher.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokeninzing the question and answer pair\n",
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
      ],
      "metadata": {
        "id": "NuXoPLD5kgZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71787de-64c2-4661-c250-a1b62b272828"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 272 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We inspect the resulting tokens and observe that each word is assigned a unique token, and that some rare words are getting split into multiple tokens. The token 101 is always the first token indicating the start of the input text, and token 102 is the separator token, which comes between the question and the answer and also at the end"
      ],
      "metadata": {
        "id": "ut-Ko7i6Zjgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspecting the resulting tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "metadata": {
        "id": "d6_sJa65koN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f79307a-de92-4040-c822-34961fd4ced2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]        101\n",
            "what       2,054\n",
            "did        2,106\n",
            "he         2,002\n",
            "say        2,360\n",
            "about      2,055\n",
            "the        1,996\n",
            "experience   3,325\n",
            "?          1,029\n",
            "[SEP]        102\n",
            "how        2,129\n",
            "much       2,172\n",
            "do         2,079\n",
            "you        2,017\n",
            "know       2,113\n",
            "about      2,055\n",
            "albert     4,789\n",
            "einstein  15,313\n",
            "?          1,029\n",
            "albert     4,789\n",
            "einstein  15,313\n",
            ",          1,010\n",
            "born       2,141\n",
            "on         2,006\n",
            "march      2,233\n",
            "14         2,403\n",
            ",          1,010\n",
            "157       17,403\n",
            "##9        2,683\n",
            "in         1,999\n",
            "germany    2,762\n",
            ",          1,010\n",
            "was        2,001\n",
            "a          1,037\n",
            "great      2,307\n",
            "scientist   7,155\n",
            "in         1,999\n",
            "the        1,996\n",
            "world      2,088\n",
            ".          1,012\n",
            "he         2,002\n",
            "was        2,001\n",
            "strange    4,326\n",
            "because    2,138\n",
            "he         2,002\n",
            "hated      6,283\n",
            "hair       2,606\n",
            "##cut     12,690\n",
            "##s        2,015\n",
            "and        1,998\n",
            "new        2,047\n",
            "clothes    4,253\n",
            ".          1,012\n",
            "he         2,002\n",
            "believed   3,373\n",
            "in         1,999\n",
            "peace      3,521\n",
            ".          1,012\n",
            "all        2,035\n",
            "his        2,010\n",
            "life       2,166\n",
            ",          1,010\n",
            "he         2,002\n",
            "hated      6,283\n",
            "war        2,162\n",
            ".          1,012\n",
            "however    2,174\n",
            ",          1,010\n",
            "his        2,010\n",
            "most       2,087\n",
            "famous     3,297\n",
            "idea       2,801\n",
            ",          1,010\n",
            "e          1,041\n",
            "=          1,027\n",
            "mc        11,338\n",
            "##2        2,475\n",
            ",          1,010\n",
            "helped     3,271\n",
            "create     3,443\n",
            "the        1,996\n",
            "world      2,088\n",
            "'          1,005\n",
            "s          1,055\n",
            "most       2,087\n",
            "dangerous   4,795\n",
            "weapon     5,195\n",
            ".          1,012\n",
            "many       2,116\n",
            "people     2,111\n",
            "think      2,228\n",
            "he         2,002\n",
            "was        2,001\n",
            "the        1,996\n",
            "smart      6,047\n",
            "##est      4,355\n",
            "person     2,711\n",
            "in         1,999\n",
            "the        1,996\n",
            "world      2,088\n",
            ".          1,012\n",
            "but        2,021\n",
            "einstein  15,313\n",
            "said       2,056\n",
            "that       2,008\n",
            "_          1,035\n",
            "what       2,054\n",
            "did        2,106\n",
            "he         2,002\n",
            "like       2,066\n",
            "?          1,029\n",
            "einstein  15,313\n",
            "liked      4,669\n",
            "learning   4,083\n",
            "sailing    8,354\n",
            ".          1,012\n",
            "he         2,002\n",
            "sailed     7,434\n",
            "in         1,999\n",
            "small      2,235\n",
            "boats      6,242\n",
            "all        2,035\n",
            "his        2,010\n",
            "life       2,166\n",
            ".          1,012\n",
            "he         2,002\n",
            "once       2,320\n",
            "joked     19,700\n",
            ",          1,010\n",
            "\"          1,000\n",
            "sailing    8,354\n",
            "is         2,003\n",
            "the        1,996\n",
            "sport      4,368\n",
            "that       2,008\n",
            "takes      3,138\n",
            "the        1,996\n",
            "least      2,560\n",
            "energy     2,943\n",
            "!            999\n",
            "\"          1,000\n",
            "when       2,043\n",
            "einstein  15,313\n",
            "was        2,001\n",
            "a          1,037\n",
            "child      2,775\n",
            ",          1,010\n",
            "his        2,010\n",
            "mother     2,388\n",
            "made       2,081\n",
            "him        2,032\n",
            "take       2,202\n",
            "violin     6,710\n",
            "lessons    8,220\n",
            ".          1,012\n",
            "at         2,012\n",
            "first      2,034\n",
            ",          1,010\n",
            "he         2,002\n",
            "didn       2,134\n",
            "'          1,005\n",
            "t          1,056\n",
            "like       2,066\n",
            "the        1,996\n",
            "violin     6,710\n",
            ".          1,012\n",
            "but        2,021\n",
            "then       2,059\n",
            "he         2,002\n",
            "learned    4,342\n",
            "to         2,000\n",
            "love       2,293\n",
            "music      2,189\n",
            "and        1,998\n",
            "became     2,150\n",
            "a          1,037\n",
            "good       2,204\n",
            "violinist  16,609\n",
            ".          1,012\n",
            "later      2,101\n",
            ",          1,010\n",
            "he         2,002\n",
            "said       2,056\n",
            ",          1,010\n",
            "\"          1,000\n",
            "love       2,293\n",
            "is         2,003\n",
            "the        1,996\n",
            "best       2,190\n",
            "teacher    3,836\n",
            ".          1,012\n",
            "\"          1,000\n",
            "why        2,339\n",
            "is         2,003\n",
            "the        1,996\n",
            "sky        3,712\n",
            "blue       2,630\n",
            "?          1,029\n",
            "in         1,999\n",
            "1910       4,976\n",
            ",          1,010\n",
            "einstein  15,313\n",
            "asked      2,356\n",
            "a          1,037\n",
            "question   3,160\n",
            "which      2,029\n",
            "many       2,116\n",
            "children   2,336\n",
            "often      2,411\n",
            "ask        3,198\n",
            ",          1,010\n",
            "\"          1,000\n",
            "why        2,339\n",
            "is         2,003\n",
            "the        1,996\n",
            "sky        3,712\n",
            "blue       2,630\n",
            "?          1,029\n",
            "\"          1,000\n",
            "after      2,044\n",
            "his        2,010\n",
            "careful    6,176\n",
            "research   2,470\n",
            ",          1,010\n",
            "he         2,002\n",
            "answered   4,660\n",
            "the        1,996\n",
            "question   3,160\n",
            "like       2,066\n",
            "this       2,023\n",
            ":          1,024\n",
            "\"          1,000\n",
            "it         2,009\n",
            "'          1,005\n",
            "s          1,055\n",
            "because    2,138\n",
            "light      2,422\n",
            "is         2,003\n",
            "made       2,081\n",
            "up         2,039\n",
            "of         1,997\n",
            "many       2,116\n",
            "colors     6,087\n",
            "including   2,164\n",
            "blue       2,630\n",
            ".          1,012\n",
            "when       2,043\n",
            "light      2,422\n",
            "travels    7,930\n",
            "to         2,000\n",
            "earth      3,011\n",
            ",          1,010\n",
            "gas        3,806\n",
            "particles   9,309\n",
            "spread     3,659\n",
            "the        1,996\n",
            "blue       2,630\n",
            "light      2,422\n",
            "all        2,035\n",
            "over       2,058\n",
            "the        1,996\n",
            "sky        3,712\n",
            ".          1,012\n",
            "\"          1,000\n",
            "his        2,010\n",
            "answer     3,437\n",
            "is         2,003\n",
            "true       2,995\n",
            "in         1,999\n",
            "physics    5,584\n",
            ".          1,012\n",
            "[SEP]        102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the number of token in question and text\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "num_seg_a = sep_idx + 1\n",
        "print(\"Number of tokens in segment A (question): \", num_seg_a)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B (answer): \", num_seg_b)"
      ],
      "metadata": {
        "id": "S-1q_yX6TVtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d9f998-b46c-4e26-85f8-79e57ea36e71"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEP token index:  9\n",
            "Number of tokens in segment A (question):  10\n",
            "Number of tokens in segment B (answer):  262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the segment ids and making sure every input token has a segment id\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "id": "SK0VnWNrUSBB"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the tokens and the segment ids will be passed to the model"
      ],
      "metadata": {
        "id": "znRu0ZUIaStT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token input_ids to represent the input and token segment_ids to differentiate \n",
        "# our segments - question and text\n",
        "output = model(torch.tensor([input_ids]),  token_type_ids = torch.tensor([segment_ids]))"
      ],
      "metadata": {
        "id": "Y95-Z6krVneF"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the start and end tokens from the output"
      ],
      "metadata": {
        "id": "sXmbS_dja1wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens with highest start and end scores\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "if answer_end >= answer_start:\n",
        "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "\n",
        "print(\"Text:\\n{}\".format(new_df[\"text\"][random_num]))\n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ],
      "metadata": {
        "id": "-0aWhakKVf-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcd95b7-66ce-4cb0-eda6-384d558a456c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            "How much do you know about Albert Einstein? Albert Einstein, born on March 14, 1579 in Germany, was a great scientist in the world. He was strange because he hated haircuts and new clothes. He believed in peace. All his life, he hated war. However, his most famous idea, E=mc2, helped create the world's most dangerous weapon . Many people think he was the smartest person in the world. But Einstein said that _ What did he like? Einstein liked learning sailing . He sailed in small boats all his life. He once joked, \"Sailing is the sport that takes the least energy!\" When Einstein was a child, his mother made him take violin lessons. At first, he didn't like the violin. But then he learned to love music and became a good violinist. Later, he said, \"Love is the best teacher.\" Why is the sky blue? In 1910, Einstein asked a question which many children often ask, \"Why is the sky blue?\" After his careful research, he answered the question like this: \"It's because light is made up of many colors including blue. When light travels to Earth, gas particles spread the blue light all over the sky.\" His answer is true in physics.\n",
            "\n",
            "Question:\n",
            "What did he say about the experience?\n",
            "\n",
            "Answer:\n",
            "\" love is the best teacher . \".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning up the answer is needed when there are multiple tokens for a word. The double hash symbols indicate that a word split into multiple tokens (separated by ##)"
      ],
      "metadata": {
        "id": "hA9WySHtb_rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning up the answer\n",
        "answer = tokens[answer_start]\n",
        "for i in range(answer_start+1, answer_end+1):\n",
        "    if tokens[i][0:2] == \"##\":\n",
        "        answer += tokens[i][2:]\n",
        "    else:\n",
        "        answer += \" \" + tokens[i]"
      ],
      "metadata": {
        "id": "eD6pNGy3VgIT"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Answer:\\n{}.\".format(answer.capitalize()))"
      ],
      "metadata": {
        "id": "ca2gMBE-WQIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0fdeeb-70cb-4baf-f545-00c0ba51c415"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "\" love is the best teacher . \".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve and print the answer to this question that we had in the training set \n",
        "answer = new_df[\"answer\"][random_num]\n",
        "answer"
      ],
      "metadata": {
        "id": "F_S7XBFCMZGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "616bf5f3-8a2b-481c-dc59-637dc3699074"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Love is the best teacher.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2"
      ],
      "metadata": {
        "id": "zeira0qecLRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization & Setup"
      ],
      "metadata": {
        "id": "PboeH1EZ5CFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import requests\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizerFast\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j3KwYPydL-bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb5b7f1-0c03-4c80-ea79-91f99f1e1c43"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a directory in Google drive\n",
        "if not os.path.exists('/content/drive/MyDrive/BERT-SQuAD'): os.mkdir('/content/drive/MyDrive/BERT-SQuAD')"
      ],
      "metadata": {
        "id": "G0Gwlj_WMf02"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the SQuAD dataset"
      ],
      "metadata": {
        "id": "vkRZRDaq5JO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the SQuAD dataset\n",
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "metadata": {
        "id": "65Wwgyg0OWIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5ae03c-62e4-4bdd-f6d7-9aa2d3039c89"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘train-v2.0.json’ already there; not retrieving.\n",
            "\n",
            "File ‘dev-v2.0.json’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset and inspecting it\n",
        "with open('train-v2.0.json', 'rb') as f:\n",
        "  squad = json.load(f)"
      ],
      "metadata": {
        "id": "GMGXyaDYOZh5"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each 'data' dict has two keys (title and paragraphs)\n",
        "squad['data'][0].keys()"
      ],
      "metadata": {
        "id": "-U4jERTmOeVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88aa28a9-7504-4494-8c96-c80b2f50e5b1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['title', 'paragraphs'])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that for each topic there are multiple paragraphs, and for each paragraph there are mutliple question and answer pairs"
      ],
      "metadata": {
        "id": "yLeVE0CYgCx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the number of topics\n",
        "len(squad['data'])"
      ],
      "metadata": {
        "id": "doO3esk5Ou2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482b3278-9b92-4010-bc72-6950e7c4b614"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(squad['data'][0]['paragraphs'][0]['qas'][0].keys())\n",
        "squad['data'][0]['paragraphs'][0]['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "l25v8ubdpjDP",
        "outputId": "2a89b4b1-a11e-447b-874e-947ed994a897"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['question', 'id', 'answers', 'is_impossible'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data in triplets of context, questions and answers\n",
        "def read_data(path):  \n",
        "  \n",
        "  with open(path, 'rb') as f:\n",
        "    squad = json.load(f)\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "\n",
        "  for group in squad['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "      context = passage['context']\n",
        "      for qa in passage['qas']:\n",
        "        question = qa['question']\n",
        "        for answer in qa['answers']:\n",
        "          contexts.append(context)\n",
        "          questions.append(question)\n",
        "          answers.append(answer)\n",
        "\n",
        "  return contexts, questions, answers"
      ],
      "metadata": {
        "id": "dCHrPhEDPNrY"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_contexts, train_questions, train_answers = read_data('train-v2.0.json')\n",
        "valid_contexts, valid_questions, valid_answers = read_data('dev-v2.0.json')"
      ],
      "metadata": {
        "id": "0znEF51bPlgL"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQnHWd_1zEh6",
        "outputId": "663c7a60-cf0c-4dc8-a7eb-d0972059c9b9"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When did Beyonce start becoming popular?',\n",
              " 'What areas did Beyonce compete in when she was growing up?',\n",
              " \"When did Beyonce leave Destiny's Child and become a solo singer?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(train_questions)} training set questions')\n",
        "print(f'There are {len(valid_questions)} dev set questions')"
      ],
      "metadata": {
        "id": "Ck3E4m9FPo5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2139f74-b4fd-44aa-f337-a6f197d28c31"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 86821 training set questions\n",
            "There are 20302 dev set questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset pre-processing"
      ],
      "metadata": {
        "id": "0C3LJeU55bvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing some data quality issues\n",
        "def add_end_idx(answers, contexts):\n",
        "  for answer, context in zip(answers, contexts):\n",
        "    gold_text = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "\n",
        "    # sometimes squad answers are off by a character or two so we fix this\n",
        "    if context[start_idx:end_idx] == gold_text:\n",
        "      answer['answer_end'] = end_idx\n",
        "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "      answer['answer_start'] = start_idx - 1\n",
        "      answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "      answer['answer_start'] = start_idx - 2\n",
        "      answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "add_end_idx(train_answers[:1000], train_contexts[:1000])\n",
        "add_end_idx(valid_answers[:100], valid_contexts[:100])"
      ],
      "metadata": {
        "id": "zHama5tjPx9v"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning BERT on SQuAD"
      ],
      "metadata": {
        "id": "0BCQsVmK56cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the model and its tokenizer (currently training on only 1000 rows as it is very time consuming)\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts[:1000], train_questions[:1000], truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(valid_contexts[:100], valid_questions[:100], truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "N02j63q0QCZJ"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings.keys()"
      ],
      "metadata": {
        "id": "qBOoOwP0QI4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88be79b8-65ea-4260-9d49-6fdad1842f6e"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualing the output of tokenizer, input ids are the token indices with padding of 0s, token_type_ids are different integers for different sequences and attention mask states which positions to give attention to while training"
      ],
      "metadata": {
        "id": "-AmGA8j9g4I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings[\"input_ids\"][0]"
      ],
      "metadata": {
        "id": "k-Zns7NT3T2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c971c2-7198-40d6-8318-86a913be1a13"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 20773,\n",
              " 21025,\n",
              " 19358,\n",
              " 22815,\n",
              " 1011,\n",
              " 5708,\n",
              " 1006,\n",
              " 1013,\n",
              " 12170,\n",
              " 23432,\n",
              " 29715,\n",
              " 3501,\n",
              " 29678,\n",
              " 12325,\n",
              " 29685,\n",
              " 1013,\n",
              " 10506,\n",
              " 1011,\n",
              " 10930,\n",
              " 2078,\n",
              " 1011,\n",
              " 2360,\n",
              " 1007,\n",
              " 1006,\n",
              " 2141,\n",
              " 2244,\n",
              " 1018,\n",
              " 1010,\n",
              " 3261,\n",
              " 1007,\n",
              " 2003,\n",
              " 2019,\n",
              " 2137,\n",
              " 3220,\n",
              " 1010,\n",
              " 6009,\n",
              " 1010,\n",
              " 2501,\n",
              " 3135,\n",
              " 1998,\n",
              " 3883,\n",
              " 1012,\n",
              " 2141,\n",
              " 1998,\n",
              " 2992,\n",
              " 1999,\n",
              " 5395,\n",
              " 1010,\n",
              " 3146,\n",
              " 1010,\n",
              " 2016,\n",
              " 2864,\n",
              " 1999,\n",
              " 2536,\n",
              " 4823,\n",
              " 1998,\n",
              " 5613,\n",
              " 6479,\n",
              " 2004,\n",
              " 1037,\n",
              " 2775,\n",
              " 1010,\n",
              " 1998,\n",
              " 3123,\n",
              " 2000,\n",
              " 4476,\n",
              " 1999,\n",
              " 1996,\n",
              " 2397,\n",
              " 4134,\n",
              " 2004,\n",
              " 2599,\n",
              " 3220,\n",
              " 1997,\n",
              " 1054,\n",
              " 1004,\n",
              " 1038,\n",
              " 2611,\n",
              " 1011,\n",
              " 2177,\n",
              " 10461,\n",
              " 1005,\n",
              " 1055,\n",
              " 2775,\n",
              " 1012,\n",
              " 3266,\n",
              " 2011,\n",
              " 2014,\n",
              " 2269,\n",
              " 1010,\n",
              " 25436,\n",
              " 22815,\n",
              " 1010,\n",
              " 1996,\n",
              " 2177,\n",
              " 2150,\n",
              " 2028,\n",
              " 1997,\n",
              " 1996,\n",
              " 2088,\n",
              " 1005,\n",
              " 1055,\n",
              " 2190,\n",
              " 1011,\n",
              " 4855,\n",
              " 2611,\n",
              " 2967,\n",
              " 1997,\n",
              " 2035,\n",
              " 2051,\n",
              " 1012,\n",
              " 2037,\n",
              " 14221,\n",
              " 2387,\n",
              " 1996,\n",
              " 2713,\n",
              " 1997,\n",
              " 20773,\n",
              " 1005,\n",
              " 1055,\n",
              " 2834,\n",
              " 2201,\n",
              " 1010,\n",
              " 20754,\n",
              " 1999,\n",
              " 2293,\n",
              " 1006,\n",
              " 2494,\n",
              " 1007,\n",
              " 1010,\n",
              " 2029,\n",
              " 2511,\n",
              " 2014,\n",
              " 2004,\n",
              " 1037,\n",
              " 3948,\n",
              " 3063,\n",
              " 4969,\n",
              " 1010,\n",
              " 3687,\n",
              " 2274,\n",
              " 8922,\n",
              " 2982,\n",
              " 1998,\n",
              " 2956,\n",
              " 1996,\n",
              " 4908,\n",
              " 2980,\n",
              " 2531,\n",
              " 2193,\n",
              " 1011,\n",
              " 2028,\n",
              " 3895,\n",
              " 1000,\n",
              " 4689,\n",
              " 1999,\n",
              " 2293,\n",
              " 1000,\n",
              " 1998,\n",
              " 1000,\n",
              " 3336,\n",
              " 2879,\n",
              " 1000,\n",
              " 1012,\n",
              " 102,\n",
              " 2043,\n",
              " 2106,\n",
              " 20773,\n",
              " 2707,\n",
              " 3352,\n",
              " 2759,\n",
              " 1029,\n",
              " 102,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings[\"token_type_ids\"][0]"
      ],
      "metadata": {
        "id": "bW5ccivv36TA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79aa6d4-cb9e-4629-f813-274270809366"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings[\"attention_mask\"][0]"
      ],
      "metadata": {
        "id": "HVFJ8SuR4zRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4360920-a5ab-442e-f640-5461bcf79925"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the number of training data samples\n",
        "no_of_encodings = len(train_encodings['input_ids'])\n",
        "print(f'We have {no_of_encodings} context-question pairs')"
      ],
      "metadata": {
        "id": "U9dorOa0REGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45033c99-6981-4e69-dd20-609a068fec50"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 1000 context-question pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding the answers in the training set for fine tuning\n",
        "def add_token_positions(encodings, answers):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "    # if start position is None, the answer passage has been truncated\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers[:1000])\n",
        "add_token_positions(valid_encodings, valid_answers[:100])"
      ],
      "metadata": {
        "id": "g_i4LZ7RRJ3L"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dataset in the format it is required for fine tuning BERT\n",
        "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "qdDN9gPBRchU"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SQuAD_Dataset(train_encodings)\n",
        "valid_dataset = SQuAD_Dataset(valid_encodings)"
      ],
      "metadata": {
        "id": "ZgEVcPOARpE_"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "xvRwLO6kRsL9"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the BERT model which we will fine tune\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "S75430sGRuof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061209ff-7bb2-4b8f-92d3-f0a7e5252ef8"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Working on {device}')"
      ],
      "metadata": {
        "id": "ALR2_y1ERxPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a508e89c-1d19-4ce7-cefd-e1165c8fc579"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tuning it per batch\n",
        "N_EPOCHS = 5\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  \n",
        "  for batch in loop:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loop.set_description(f'Epoch {epoch+1}')\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "BLb91b0oR6Rv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25ebbb9-ddb2-483b-ce4a-405a77547dde"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s, loss=3.69]\n",
            "Epoch 2: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s, loss=1.58]\n",
            "Epoch 3: 100%|██████████| 125/125 [01:32<00:00,  1.35it/s, loss=0.751]\n",
            "Epoch 4: 100%|██████████| 125/125 [01:32<00:00,  1.36it/s, loss=0.279]\n",
            "Epoch 5: 100%|██████████| 125/125 [01:32<00:00,  1.36it/s, loss=0.327]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the performance\n",
        "model.eval()\n",
        "\n",
        "acc = []\n",
        "\n",
        "for batch in tqdm(valid_loader):\n",
        "  with torch.no_grad():\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_true = batch['start_positions'].to(device)\n",
        "    end_true = batch['end_positions'].to(device)\n",
        "    \n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "\n",
        "    acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "    acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "\n",
        "acc = sum(acc)/len(acc)"
      ],
      "metadata": {
        "id": "osjV8pPYR7ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5601a455-7595-4629-bf02-701463476323"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:02<00:00,  4.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "5HtohT6P6oFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6617dcfe-b051-4b6d-9463-0cb1cb0d0e87"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5721153846153846"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework assignment"
      ],
      "metadata": {
        "id": "iVyMJBG3gJJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Fine-tune BERT for question answering on the CoQA dataset using the same process as shown in Part 2 for the SQuAD dataset."
      ],
      "metadata": {
        "id": "64EPp8N-gMAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier we have downloaded Coqa train set converted it into dataframe named new_df, so we will just make a copy of it and use it."
      ],
      "metadata": {
        "id": "NFE9A23b0yR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For CoQa dev dataset, we just need its key and values from text, question and answer."
      ],
      "metadata": {
        "id": "qjDusMbY0-Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json\n",
        "!wget -nc https://downloads.cs.stanford.edu/nlp/data/coqa/coqa-dev-v1.0.json\n",
        "with open('coqa-train-v1.0.json', 'rb') as f:\n",
        "  coqa_train = json.load(f)\n",
        "with open('coqa-dev-v1.0.json', 'rb') as f:\n",
        "  coqa_test = json.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XpCfTFN_yUA",
        "outputId": "ed6d00fc-a6f5-487b-ab8b-3056ec26aff4"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘coqa-train-v1.0.json’ already there; not retrieving.\n",
            "\n",
            "File ‘coqa-dev-v1.0.json’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each 'data' dict has seven keys \n",
        "coqa_test['data'][0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfh6_Z3jACbU",
        "outputId": "2ec340c9-d674-4c87-eab8-635c0bcccec7"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['source', 'id', 'filename', 'story', 'questions', 'answers', 'additional_answers', 'name'])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that some of the answer are unknown in this CoQa Dataset, so we will remove those answers and questions related them.\n"
      ],
      "metadata": {
        "id": "RQVWgpL4M7JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coqa_train['data'][0]['answers'][13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM_pAQtrAv8F",
        "outputId": "f19293d8-5c6e-4d9a-fd4f-24fd32cd2968"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'span_start': -1,\n",
              " 'span_end': -1,\n",
              " 'span_text': 'unknown',\n",
              " 'input_text': 'unknown',\n",
              " 'turn_id': 14,\n",
              " 'bad_turn': 'true'}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data in triplets of context, questions and answers\n",
        "def read_data_coqa(path):  \n",
        "  with open(path, 'rb') as f:\n",
        "    squad = json.load(f)\n",
        "  skip_list=[]\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  c=-1\n",
        "  count=-1\n",
        "  for group in squad['data']:\n",
        "    for i in group['answers']:\n",
        "      c+=1\n",
        "      if i['span_start']==-1:         # added condition to remove -1 values\n",
        "        skip_list.append(c)\n",
        "        continue\n",
        "      answers.append(i)\n",
        "  for group in squad['data']:\n",
        "    for i in group['questions']:\n",
        "      count+=1\n",
        "      if count not in skip_list:\n",
        "        context = group['story']\n",
        "        question=i['input_text']\n",
        "        contexts.append(context)\n",
        "        questions.append(question)\n",
        "\n",
        "  s=skip_list\n",
        "  return contexts, questions, answers\n",
        "\n",
        "train_contexts_coqa,train_questions_coqa,train_answers_coqa=read_data_coqa('coqa-train-v1.0.json')\n",
        "test_contexts_coqa,test_questions_coqa,test_answers_coqa=read_data_coqa('coqa-dev-v1.0.json')\n"
      ],
      "metadata": {
        "id": "W38phkEEog8t"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_contexts_coqa),len(train_questions_coqa),len(train_answers_coqa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTVeLENsI-uy",
        "outputId": "7b5c9af7-c7de-4a78-a27b-2a6537afec8c"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107286 107286 107286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(train_questions_coqa)} training set questions')\n",
        "print(f'There are {len(test_questions_coqa)} dev set questions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkKfEPjIogvm",
        "outputId": "cdd3e2c7-d1eb-40e9-9ec9-c072d33da8ba"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 107286 training set questions\n",
            "There are 7918 dev set questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing some data quality issues\n",
        "def add_end_idx_coqa(answers, contexts):\n",
        "  for answer, context in zip(answers, contexts):\n",
        "    gold_text = answer['input_text']\n",
        "    start_idx = answer['span_start']\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "\n",
        "    # sometimes squad answers are off by a character or two so we fix this\n",
        "    if context[start_idx:end_idx] == gold_text:\n",
        "      answer['span_end'] = end_idx\n",
        "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "      if start_idx>1:\n",
        "        answer['span_start'] = start_idx - 1\n",
        "        answer['span_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "      if start_idx>2:\n",
        "        answer['span_start'] = start_idx - 2\n",
        "        answer['span_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "add_end_idx_coqa(train_answers_coqa[:1000], train_contexts_coqa[1000])\n",
        "add_end_idx_coqa(test_answers_coqa[:100], test_contexts_coqa[:100])"
      ],
      "metadata": {
        "id": "YjNctG57ogtP"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the model and its tokenizer (currently training on only 1000 rows as it is very time consuming)\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings_coqa = tokenizer(train_contexts_coqa[:1000], train_questions_coqa[:1000], truncation=True, padding=True)\n",
        "test_encodings_coqa = tokenizer(test_contexts_coqa[:100], test_questions_coqa[:100], truncation=True, padding=True)\n",
        "# printing the number of training data samples\n",
        "no_of_encodings = len(train_encodings_coqa['input_ids'])\n",
        "print(f'We have {no_of_encodings} context-question pairs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiH5n3Iiogqn",
        "outputId": "fe6c2cd5-65ed-4c0c-8f38-ea45a5b20f6a"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 1000 context-question pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding the answers in the training set for fine tuning\n",
        "def add_token_positions_coqa(encodings, answers):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['span_start']))\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['span_end'] - 1))\n",
        "\n",
        "    # if start position is None, the answer passage has been truncated\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions_coqa(train_encodings_coqa, train_answers_coqa[:1000])\n",
        "add_token_positions_coqa(test_encodings_coqa, test_answers_coqa[:100])"
      ],
      "metadata": {
        "id": "XK5ef36AGSKX"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dataset in the format it is required for fine tuning BERT\n",
        "class CoQa_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "ba6ki5mqGSG5"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CoQa_Dataset(train_encodings_coqa)\n",
        "valid_dataset = CoQa_Dataset(test_encodings_coqa)\n",
        "# Define the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8)\n",
        "# loading the BERT model which we will fine tune\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "# checking the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Working on {device}')\n",
        "# Fine tuning it per batch\n",
        "N_EPOCHS = 5\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  \n",
        "  for batch in loop:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loop.set_description(f'Epoch {epoch+1}')\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "bldC7zvUGSEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabbc073-b1a1-45b7-c033-45bf3aec6ebb"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 125/125 [01:41<00:00,  1.23it/s, loss=3.91]\n",
            "Epoch 2: 100%|██████████| 125/125 [01:41<00:00,  1.23it/s, loss=4.8]\n",
            "Epoch 3: 100%|██████████| 125/125 [01:42<00:00,  1.22it/s, loss=2.23]\n",
            "Epoch 4: 100%|██████████| 125/125 [01:41<00:00,  1.23it/s, loss=2.09]\n",
            "Epoch 5: 100%|██████████| 125/125 [01:41<00:00,  1.23it/s, loss=1.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the performance\n",
        "model.eval()\n",
        "\n",
        "acc = []\n",
        "\n",
        "for batch in tqdm(valid_loader):\n",
        "  with torch.no_grad():\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_true = batch['start_positions'].to(device)\n",
        "    end_true = batch['end_positions'].to(device)\n",
        "    \n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "    end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "\n",
        "    acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "    acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "\n",
        "acc = sum(acc)/len(acc)\n",
        "acc"
      ],
      "metadata": {
        "id": "0OWzuV75ogil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b756d33d-388e-45ec-9ccd-b669949400aa"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:03<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1346153846153846"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: Import the BERT model fine-tuned for classification and test its performance on any text classification dataset such as the twitter dataset."
      ],
      "metadata": {
        "id": "oxVhn7RbgWnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using IMDB Dataset for sentiment analysis using BERT model"
      ],
      "metadata": {
        "id": "wJ7VKCKm0bbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc6Fwi8_0seN",
        "outputId": "4eb7b2f4-6503-441b-8b3d-a1bad93350fb"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import random\n",
        "from transformers import pipeline\n",
        "import evaluate\n",
        "from evaluate import evaluator\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "imdb = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "heT4mxyeTtq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b8a16fd69d4f4ccaa30109be6ac50880",
            "6c2e6af6e4bf417f9c744d48d6395412",
            "1514114962e14efb8a2034cf9e6bdf06",
            "01c1a958b4c6428ea2d40f85d14728dd",
            "7bf0401b5fa74ca1bd1134b9e8fec221",
            "495cafb13dbc405c87bf8860e06c86d3",
            "4e2c23c2e5e447558b65e88fcbab29db",
            "9f90c14545c6473d98798556e2219a9f",
            "ab3f8ea56a584aeaa126dec8f0edb1c8",
            "ad47f6154fe145c6856534c5b3597282",
            "cb190399aa9b4c349e37853555a7b5f2"
          ]
        },
        "outputId": "7f4d9fa0-71fb-4428-e05b-e56e69d73f02"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8a16fd69d4f4ccaa30109be6ac50880"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "max_length = 512\n",
        "def preprocess_fn(a):\n",
        "    return tokenizer(a[\"text\"], truncation=True)\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "tokenized_imdb = imdb.map(preprocess_fn, batched=True)"
      ],
      "metadata": {
        "id": "SyXwD950Ttnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52,
          "referenced_widgets": [
            "4aa22860df54419ba25a20b865cd8e32",
            "5ba6c4dbb5554d048fea87dd0b6d6cec",
            "8f3fda2197b6447ab609f7d7e4dfaf9b",
            "9e66885cd12d4c7aa3a6862dfcde2b4b",
            "85e52fb30ac74f85a2cd5a6adfd96ecc",
            "5e167b521be64744bb14ea93d45bf6e7",
            "67cd814d4b7e424eb34d1630d40c2022",
            "0dabbe81bbfe44de9942323ab28b94ca",
            "963d8cf619924c088bd1057fddc1644e",
            "12608a66b9c94035928251aa31cb3d1a",
            "01bf3d186abd48559d658ed92d65fe92"
          ]
        },
        "outputId": "049f4c36-0d59-470b-c29a-742f6e5ade7e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-e4a94efb40501b36.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4aa22860df54419ba25a20b865cd8e32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-11e59d6a16cf13ac.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
      ],
      "metadata": {
        "id": "LFcKD0zQTtlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953ec53a-d18a-4fc9-ae13-9ddc43c15942"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIdvEaBMMfuX",
        "outputId": "1eae76a6-004c-4a19-d7d3-f724eaa3aa61"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<transformers.trainer.Trainer at 0x7f1b2bf9c460>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTJuKb7h2ihu",
        "outputId": "be621126-c98c-4f0b-a5d5-f8677b771042"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train', 'test', 'unsupervised'])"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets just test the current trained model on random \"unsupervised\" set of IMDB dataset. For example 11th paragraph"
      ],
      "metadata": {
        "id": "Huwrag2W2jkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['unsupervised'][11]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "wDkxhwNf23PX",
        "outputId": "fac7d8c2-6595-4e72-935f-848b32fdf27d"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A lot of people criticize this movie as just being too dumb, but I'm just saying, give this movie a chance. It may not be the best film ever made, but at least it has clever gags in it and also includes a very young John Candy. I saw clips of the movie on the internet and I was amazed on how they were funny, especially the scene when Candy unintentionally almost destroys the entire police station while trying to find a pencil or when he hilariously fails his shooting test in the gun range. Since this movie was barely released in the U.S., you may want to track down a copy of it before it disappears completely. Also look for Peter Cook, Mickey Rooney, and Dick Emery, in what was to be his last film role, in the cast, as well.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = str(imdb['unsupervised'][11]['text'])\n",
        "\n",
        "model_pred = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device = 0)\n",
        "model_pred(input_text)"
      ],
      "metadata": {
        "id": "4auP6jRRTtfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d06a69-e48b-4f05-ae4e-a311548b3ffe"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.5319817662239075}]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3: Fine-tune the BERT model from Exercise 2 on the text classification dataset you used for testing (in Exercise 2) and evaluate its performance (on a test set from the dataset that you set aside prior to fine tuning the model)"
      ],
      "metadata": {
        "id": "aFq-55kTglbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test our above model on the IMDB test set"
      ],
      "metadata": {
        "id": "7HcueGts3gOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_evaluator = evaluator(\"text-classification\")\n",
        "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\n",
        "final = our_evaluator.compute(model_or_pipeline=model,data=data,tokenizer=tokenizer,metric=\"accuracy\",label_mapping=label2id)\n",
        "final['accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEXRqycV15De",
        "outputId": "801993e7-f53f-4f59-ba66-b4da19e478f6"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-c1eaa46e94dfbfd3.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.487"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are getting preety decent accuracy"
      ],
      "metadata": {
        "id": "jKn-ZeBW14tA"
      }
    }
  ]
}